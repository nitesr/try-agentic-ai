{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027af072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- text-embedding-nomic-embed-text-v1.5\n",
      "- qwen/qwen3-4b-2507\n",
      "- meta-llama-3.1-8b-instruct\n",
      "- text-embedding-bge-small-en-v1.5\n",
      "- text-embedding-all-minilm-l6-v2-embedding\n",
      "- deepseek/deepseek-r1-0528-qwen3-8b\n",
      "- qwen/qwen3-4b-thinking-2507\n",
      "- llama-3.2-3b-instruct\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# List available models from the local LLM server\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:1234/v1/models\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"Available models:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"- {model.get('id', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"Failed to get models: {response.status_code} - {response.text}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error connecting to local LLM server: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490c8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qwen/qwen3-4b-2507\", # deepseek/deepseek-r1-0528-qwen3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8bb55fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f60aac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a52bcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in San Francisco?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c96e3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, user is asking about the current weather in San Francisco. This seems like a straightforward query that requires external data.\\n\\nThe user hasn\\'t specified temperature unit preference, so I\\'ll default to Celsius since that\\'s the international standard. However, if they use a tool without specifying unit later, it might default to something else - better stick with simple request.\\n\\nLooking at the available tool: \"get_current_weather\" matches perfectly. Location must be specified exactly - including state if needed, which in this case is California so \"San Francisco, CA\".\\n\\nThe system requires strict formatting for tool calls. No room for additional text after the final tool request, and must use exactly  tags. Must ensure the JSON structure matches what the system expects.\\n\\nConsidering this is likely a simple query without emotional context, just need to provide factual weather information. No signs the user needs this for anything specific - could be planning outdoor activities or just curious.\\n\\nResponse will stay purely technical, with no additional information unless the user asks for elaboration. Strict rules about tool interaction in this system.\\n</think>\\n', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='363199735', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_current_weather'), type='function')]))]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"dummy-key\"  # Local LLM servers often don't require a real API key\n",
    ")\n",
    "\n",
    "# Test the connection and make a function call\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,  # Use whatever model name your local server expects\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "print(response.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4236cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='<think>\\nOkay, user is asking about the current weather in San Francisco. This seems like a straightforward query that requires external data.\\n\\nThe user hasn\\'t specified temperature unit preference, so I\\'ll default to Celsius since that\\'s the international standard. However, if they use a tool without specifying unit later, it might default to something else - better stick with simple request.\\n\\nLooking at the available tool: \"get_current_weather\" matches perfectly. Location must be specified exactly - including state if needed, which in this case is California so \"San Francisco, CA\".\\n\\nThe system requires strict formatting for tool calls. No room for additional text after the final tool request, and must use exactly  tags. Must ensure the JSON structure matches what the system expects.\\n\\nConsidering this is likely a simple query without emotional context, just need to provide factual weather information. No signs the user needs this for anything specific - could be planning outdoor activities or just curious.\\n\\nResponse will stay purely technical, with no additional information unless the user asks for elaboration. Strict rules about tool interaction in this system.\\n</think>\\n', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='363199735', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_current_weather'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1aa0412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let me handle this user query. The user is asking about the weather in San Francisco.\\n\\nFirst, I need to determine if this requires any tools. Available tools include a get_current_weather tool that takes location and optional unit parameters.\\n\\nSan Francisco seems like the perfect case for this tool. Location is clear, \"San Francisco\", CA. I can include the unit parameter as well.\\n\\nConsidering whether to use Celsius or Fahrenheit in response, since both are common measurements. The temperature details would depend on the actual data, but that\\'s something I\\'ll get from the tool.\\n\\nFor such queries about current conditions or forecasts, using this weather tool makes sense as it provides accurate information based on data sources.\\n\\nLooking at the format, I need to provide a tool call block with proper syntax. Since this requires only one response, I should keep it simple without extra text.\\n\\nThis seems like a straightforward request that doesn\\'t require additional context or follow-up.\\n</think>\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07ae8bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageFunctionToolCall(id='708015470', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_current_weather'), type='function')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb2e7e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'San Francisco'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(response_message.tool_calls[0].function.arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6703cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"San Francisco\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = json.loads(response_message.tool_calls[0].function.arguments)\n",
    "get_current_weather(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18335b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='<think>\\nFirst, the user said \"hi!\" which is a greeting. I need to check if this requires any tools or just a normal response.\\n\\nLooking at the available tools, there\\'s only \"get_current_weather\" listed. But this message doesn\\'t mention anything about weather or other actions like sending emails or opening browsers.\\n\\nThe tool description is \"Get the current weather in a given location\", and it requires one required parameter: \"location\". The user\\'s message is just \"hi!\", which doesn\\'t provide any location or context that suggests a weather check.\\n\\nAdditionally, the rules state:\\n- Only use listed tools if needed.\\n- If calling tools, end with tool calls (no text after).\\n- Answer normally if no tools are needed.\\n\\nSo, since this is a simple greeting and there\\'s no indication that a tool should be used, I should respond normally without calling any tools.\\n\\nMy response should be helpful and engaging. Perhaps I could say hello back or ask how I can assist them.\\n\\nBut the system prompt emphasizes that if tools are needed, use the exact format. Here, no tools seem necessary.\\n\\nAlso, I must not output any [TOOL_RESULT] or similar tags myself if no tool is called.\\n\\nSo, for this \"hi!\", I\\'ll just give a normal response. Maybe: \"Hello! How can I help you today?\" or something inviting.\\n\\nI should confirm that this doesn\\'t trigger any tools. No weather-related query here, just a casual greeting.\\n\\nIn the examples:\\n- \"Check Paris weather\" directly prompts for a tool call.\\n- \"Email John and open browser\" would require multiple tools, but the rule says \"One tool per block\", so perhaps that\\'s split into two TOOL_REQUESTs.\\n\\nBut for \"hi!\", it doesn\\'t match any tool directly. So, no need to call a tool.\\n\\nFinal decision: Respond normally with a greeting or offer assistance.\\n</think>\\nHello! How can I assist you today? If you have any questions, feel free to ask.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[])\n"
     ]
    }
   ],
   "source": [
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\"\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f61520ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello! How can I assist you today? If you have any questions, feel free to ask.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content = response.choices[0].message.content\n",
    "response_content.split('</think>', 1)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2f753",
   "metadata": {},
   "source": [
    "# force not to use tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78643e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-61dc6z6k3tj5ky677ydy05', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, user is asking about the weather in Dallas. Hmm, this seems like a straightforward question at first glance, but let me think deeper.\\n\\nThe user probably wants to know the current weather conditions to plan something. Maybe they\\'re local and need real-time info, or a traveler checking forecasts before departure. Could also be someone setting up a meeting or event.\\n\\nBut wait, why is the user asking an assistant instead of checking a weather app or website? Maybe they\\'re in a situation where internet access isn\\'t available, like during travel or at home. Or perhaps they want human interpretation of weather patterns rather than raw data.\\n\\nI don\\'t have live access to current weather, but I can provide accurate Dallas-specific information. The best approach is to explain that limitation while offering concrete ways they could find this data.\\n\\nShould mention notable Dallas seasons too - winter chill and summer heat are famous in that area. Might help the user understand why they\\'re being asked about Dallas specifically.\\n\\nThe response should be practical - direct them to reliable sources without making assumptions. Maybe suggest several options from different providers so they can compare.\\n\\nNeed to keep it helpful but not overbearing. Break it down into action steps rather than just saying \"I don\\'t know.\" And maintain a warm tone - it\\'s better to assist even when I can\\'t provide the answer directly.\\n\\nOh, and important to specify it\\'s \"right now\" since weather changes constantly. Don\\'t just say \"weather in Dallas today.\"\\n</think>\\nI can check the current weather conditions for Dallas, Texas. Please note that my search results are up-to-date based on information typically current as of June 2024.\\n\\nWant me to find it for you right now?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]))], created=1755746283, model='deepseek/deepseek-r1-0528-qwen3-8b', object='chat.completion', service_tier=None, system_fingerprint='deepseek/deepseek-r1-0528-qwen3-8b', usage=CompletionUsage(completion_tokens=346, prompt_tokens=10, total_tokens=356, completion_tokens_details=None, prompt_tokens_details=None), stats={})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nI can check the current weather conditions for Dallas, Texas. Please note that my search results are up-to-date based on information typically current as of June 2024.\\n\\nWant me to find it for you right now?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"whats the weather in Dallas?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"none\"\n",
    ")\n",
    "print(response)\n",
    "response.choices[0].message.content.split('</think>', 1)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1917b",
   "metadata": {},
   "source": [
    "# force to use the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0c8d084",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': \"Invalid tool_choice type: 'object'. Supported string values: none, auto, required\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m messages= [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhi!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     }\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = client.chat.completions.create(\n\u001b[32m      8\u001b[39m     model=model_name,\n\u001b[32m      9\u001b[39m     messages=messages,\n\u001b[32m     10\u001b[39m     tools=tools,\n\u001b[32m     11\u001b[39m     tool_choice={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mget_current_weather\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1149\u001b[39m         body=maybe_transform(\n\u001b[32m   1150\u001b[39m             {\n\u001b[32m   1151\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1152\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1153\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1154\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1155\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1156\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1157\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1158\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1159\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1160\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1161\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1162\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1163\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1164\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1165\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1166\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1168\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1169\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1174\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1175\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1185\u001b[39m             },\n\u001b[32m   1186\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   1187\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1188\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   1189\u001b[39m         ),\n\u001b[32m   1190\u001b[39m         options=make_request_options(\n\u001b[32m   1191\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1192\u001b[39m         ),\n\u001b[32m   1193\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1194\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1195\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m   1196\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': \"Invalid tool_choice type: 'object'. Supported string values: none, auto, required\"}"
     ]
    }
   ],
   "source": [
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\"\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}}\n",
    ")\n",
    "\n",
    "print(response)\n",
    "#response.choices[0].message.content.split('</think>', 1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78377156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8dth418a6ld9767xkntdhr', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=\"<think>\\nOkay, let's break this down. The user is asking about the weather in San Francisco, which means they need a tool call to get that information.\\n\\nFirst thought: The current weather is definitely the best way to answer this. We have exactly one tool available for weather queries.\\n\\nSecond thought: The user didn't specify a unit, but the default behavior of tools should handle this. Looking at how other assistants handled similar queries, they often assumed metric units for international locations.\\n\\nThird thought: San Francisco is a great choice of location, so I'll keep this simple by just calling the tool with that one parameter.\\n\\nFourth thought: The response should be concise, state we're checking the weather, then show exactly the tool call format without any extra text after it.\\n\\nYes, this matches our available tools and response rules. The user probably wants accurate current weather to plan something today, maybe an outdoor activity or just curious about the forecast.\\n</think>\\nI want to check if it's a good day for outdoor activities in San Francisco. Let me verify the current weather.\\n\\n\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='291080387', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_current_weather'), type='function')]))], created=1755747572, model='deepseek/deepseek-r1-0528-qwen3-8b', object='chat.completion', service_tier=None, system_fingerprint='deepseek/deepseek-r1-0528-qwen3-8b', usage=CompletionUsage(completion_tokens=245, prompt_tokens=384, total_tokens=629, completion_tokens_details=None, prompt_tokens_details=None), stats={})\n",
      "ChatCompletion(id='chatcmpl-v6j15y79pordevkz5ir71s', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, the user wants to know what happens next after seeing that tool result about San Francisco\\'s weather. Let me think about this query.\\n\\nThe key points from the previous interaction are:\\n- The user asked \"What\\'s the weather like in San Francisco?\"\\n- I responded appropriately, requesting the tool to get current weather data.\\n\\nNow there\\'s a tool result showing:\\n- Location: San Francisco, CA\\n- Temperature: 72°F (~22°C)\\n- Forecast includes \"sunny\" and \"windy\"\\n\\nThe user didn\\'t specify any immediate need in this latest message, so I should provide value based on what seems useful given the information.\\n\\nHmm... let\\'s break down this scenario. The user appears to be planning an outdoor activity, given my earlier \"good day for outdoor activities\" comment. San Francisco\\'s weather during summer months often has nice days but can be foggy or windy.\\n\\nWhat would a thoughtful assistant do with this info? First, confirm that indeed this is good for outdoors - the temperature seems pleasant (~22°C) and forecast suggests mostly clear.\\n\\nThen, what\\'s valuable to add? The wind part might matter depending on the activity. If it\\'s hiking or cycling, strong winds could be relevant even if just \"windy\" is mentioned.\\n\\nShould I consider any potential follow-up questions the user might have about this weather data? They could be planning a specific outing or event.\\n\\nThe assistant\\'s role here is to help the user make better decisions with this information. The temperature and forecast provide enough context to suggest it might be a good day for outdoors, but the wind could add nuance.\\n\\nI should note that while conditions look nice, wind might still be a factor to consider depending on what the user plans. This could encourage them to specify their needs.\\n\\nThe temperature conversion in my response assumes we\\'re reaching back into that context, but I should verify if the user is indeed interested in both metric and imperial measurements.\\n\\nThe tool result doesn\\'t specify a date, so the weather details might be current or upcoming. That could potentially affect how helpful this information would be.\\n\\nThe temperature range is implied rather than explicitly stated, so it might not reflect actual comfort levels. The forecast also doesn\\'t mention humidity or other factors that could affect outdoor plans.\\n\\nThe city name being \"San Francisco, CA\" suggests the tool requires state or region specification. That seems correct.\\n\\nThe assistant could provide more specific advice based on what the user actually plans to do. The temperature alone might not be enough information without knowing the day length or intensity of planned activities.\\n\\nThe wind data isn\\'t contextualized in terms of its effect. A \"windy\" day could be okay for some outdoor activities like sailing or wind-surfing, but might not suit others.\\n\\nPerhaps I should address multiple scenarios: a sunny day with wind would be good for outdoor spots but might not encourage intense activity like running or climbing.\\n\\nThe response seems to assume that the user wants general advice about whether outdoor activities are suitable, but without knowing what those activities are.\\n\\nThe assistant\\'s approach might be a bit generic since the forecast doesn\\'t provide specific details about wind strength or direction.\\n\\nThe temperature conversion seems reasonable, but I wonder if the user might prefer one system over the other depending on their location.\\n\\nThe response doesn\\'t account for potential variations in daily routine or personal tolerance to weather conditions. Someone might be okay with wind even if it\\'s \"very windy\".\\n\\nI think the assistant has a solid foundation but could be more specific. The user might not have mentioned their plans, so the response is rather general.\\n\\nThe core advice stands though: don\\'t wear light layers without wind protection. This addresses the most common outdoor scenarios.\\n\\nPotential follow-up questions might include:\\n- What exactly do you plan to do outside?\\n- Will the wind interfere with swimming or boating?\\n\\nBut since the user didn\\'t ask, I should stick to providing information they might find useful.\\n\\nThe assistant could have asked clarifying questions rather than making assumptions, which would improve the interaction. But given current constraints, this seems like a reasonable approach.\\n\\nI should also consider that while the temperature might be comfortable today, it could change later in the day. But without more information about heat index or peak temperatures, this might be overcomplicating things.\\n\\nThe response doesn\\'t account for the user\\'s current location or whether they\\'re already in San Francisco. They might be planning ahead.\\n\\nIn summary, the assistant\\'s response provides a solid interpretation of the weather data with practical advice about packing and planning. It could potentially be more contextual depending on what else the user revealed, but it\\'s a good starting point.\\n</think>\\nOkay,SFO_72F_Windy.\\n\\nHere is the interpretation in context:\\n\\n*   Sunny: Good visibility, minimal rain chance.\\n*   Windy: Conditions might be breezy later. Don\\'t forget wind protection.\\n\\nThat forecast suggests **good conditions for outdoor activities** today, with a pleasant temperature of 72°F (which is approximately ~22°C).\\n\\nHowever, keep in mind the wind factor depending on what outdoor plans you have.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]))], created=1755747579, model='deepseek/deepseek-r1-0528-qwen3-8b', object='chat.completion', service_tier=None, system_fingerprint='deepseek/deepseek-r1-0528-qwen3-8b', usage=CompletionUsage(completion_tokens=1031, prompt_tokens=488, total_tokens=1519, completion_tokens_details=None, prompt_tokens_details=None), stats={})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOkay,SFO_72F_Windy.\\n\\nHere is the interpretation in context:\\n\\n*   Sunny: Good visibility, minimal rain chance.\\n*   Windy: Conditions might be breezy later. Don't forget wind protection.\\n\\nThat forecast suggests **good conditions for outdoor activities** today, with a pleasant temperature of 72°F (which is approximately ~22°C).\\n\\nHowever, keep in mind the wind factor depending on what outdoor plans you have.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in San Francisco?\"\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "response_message = response.choices[0].message\n",
    "messages.append(response_message)\n",
    "\n",
    "\n",
    "tool_call = response_message.tool_calls[0]\n",
    "function_name = tool_call.function.name\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "observation = get_current_weather(args)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"name\": function_name,\n",
    "    \"content\": observation\n",
    "})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "print(response)\n",
    "response.choices[0].message.content.split('</think>', 1)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
