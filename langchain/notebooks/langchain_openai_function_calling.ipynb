{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b721e43",
   "metadata": {},
   "source": [
    "# Try Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5578b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str = Field(description=\"The name of the user\")\n",
    "    age: int = Field(description=\"The age of the user\")\n",
    "    email: str = Field(description=\"The email of the user\")\n",
    "\n",
    "class Class(BaseModel):\n",
    "    students: list[User] = Field(description=\"The students of the class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad64e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Joe' age=32 email='joe@example.com'\n"
     ]
    }
   ],
   "source": [
    "foo_user = User(name=\"Joe\", age=32, email=\"joe@example.com\")\n",
    "\n",
    "print(foo_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060cea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students=[User(name='Joe', age=32, email='joe@example.com')]\n"
     ]
    }
   ],
   "source": [
    "foo_class = Class(students=[User(name=\"Joe\", age=32, email=\"joe@example.com\")])\n",
    "print(foo_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c50276",
   "metadata": {},
   "source": [
    "# Use pydantic to create OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5e1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call with airport code to get the weather at the airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get the weather at the airport\")\n",
    "\n",
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call with artist name to get the artist's information\"\"\"\n",
    "    artist_name: str = Field(description=\"artist name to get the artist's information\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111528ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/s1_3z0zx20z4kvwtxs51stc80000gp/T/ipykernel_58253/84629652.py:3: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  weather_function = convert_pydantic_to_openai_function(WeatherSearch)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch',\n",
       " 'description': 'Call with airport code to get the weather at the airport',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get the weather at the airport',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)\n",
    "weather_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737ed3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "\n",
    "class AnswerStrOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # Split on </think> and take the content after it\n",
    "        if '</think>' in text:\n",
    "            return text.split('</think>', 1)[-1].strip()\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb021140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'WeatherSearch', 'args': {'airport_code': 'LAX'}, 'id': '407533289', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create ChatOpenAI instance\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528-qwen3-8b\", \n",
    "    api_key=\"dummy\",\n",
    "    temperature=0)\n",
    "\n",
    "# Bind the WeatherSearch function to the LLM\n",
    "llm_with_tools = llm.bind_tools([WeatherSearch, ArtistSearch])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "output_parser = AnswerStrOutputParser()\n",
    "\n",
    "# Invoke the LLM with a weather query\n",
    "chain = prompt | llm_with_tools \n",
    "response = chain.invoke({\"input\": \"What's the weather like at LAX airport?\"})\n",
    "\n",
    "print(response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a700c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'ArtistSearch', 'args': {'artist_name': 'The Beatles'}, 'id': '122762446', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm_with_tools \n",
    "response = chain.invoke({\"input\": \"What are three songs of the artist 'The Beatles'?\"})\n",
    "\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm_with_tools | output_parser\n",
    "response = chain.invoke({\"input\": \"Hi!\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82dafe",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2b503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': 'Tag sentiment and language',\n",
       " 'parameters': {'properties': {'sentiment': {'description': 'sentiment for e.g. positive, negative, neutral',\n",
       "    'type': 'string'},\n",
       "   'language': {'description': 'language code for e.g. en, it, es',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag sentiment and language\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment for e.g. positive, negative, neutral\")\n",
    "    language: str = Field(description=\"language code for e.g. en, it, es\")\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df7a86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'sentiment': 'positive', 'language': 'en'}, 'type': 'Tagging'},\n",
       " {'args': {'sentiment': 'positive', 'language': 'en'}, 'type': 'Tagging'},\n",
       " {'args': {'sentiment': 'positive', 'language': 'en'}, 'type': 'Tagging'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "# Create ChatOpenAI instance\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528-qwen3-8b\", \n",
    "    api_key=\"dummy\",\n",
    "    temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([Tagging], tool_choice=\"required\")\n",
    "output_parser = JsonOutputToolsParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Think carefully, and then tag the text as instructed.'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "chain = prompt | llm_with_tools | output_parser\n",
    "response = chain.invoke({\"input\": \"The weather here is sunny and warm.\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e041ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'sentiment': 'positive', 'language': 'en'}, 'type': 'Tagging'},\n",
       " {'args': {'sentiment': 'positive', 'language': 'en'}, 'type': 'Tagging'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"I love LangChain!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cdd602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'sentiment': 'negative', 'language': 'it'}, 'type': 'Tagging'},\n",
       " {'args': {'sentiment': 'negative', 'language': 'it'}, 'type': 'Tagging'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"non mi piace il cibo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd3ca4",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f2079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a Person\"\"\"\n",
    "    name: str = Field(description=\"name of the person\")\n",
    "    age: Optional[int] = Field(description=\"age of the person\")\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    persons: List[Person] = Field(description=\"list of persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b21a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import httpx\n",
    "\n",
    "# Enable httpx debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "httpx_logger = logging.getLogger(\"httpx\")\n",
    "httpx_logger.setLevel(logging.WARN)\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Create a custom handler to see the HTTP calls clearly\n",
    "class HTTPHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        if DEBUG and ('HTTP' in record.getMessage() or 'request' in record.getMessage().lower()):\n",
    "            print(f\"ðŸŒ {record.getMessage()}\")\n",
    "\n",
    "http_handler = HTTPHandler()\n",
    "httpx_logger.addHandler(http_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979f4672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'persons': [{'name': 'John', 'age': 30}]}, 'type': 'Information'},\n",
       " {'args': {'persons': [{'name': 'John', 'age': 30}]}, 'type': 'Information'},\n",
       " {'args': {'persons': [{'name': 'John', 'age': 30}]}, 'type': 'Information'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "\n",
    "extraction_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528-qwen3-8b\", # \"meta-llama-3.1-8b-instruct\", \n",
    "    api_key=\"dummy\",\n",
    "    temperature=0)\n",
    "\n",
    "extraction_llm_with_tools = extraction_llm.bind_tools([Person, Information])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Extract the information from the text and call the tools. DONOT guess the information.'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "extraction_chain = prompt | extraction_llm_with_tools | JsonOutputToolsParser()\n",
    "extraction_chain.invoke({\"input\": \"John is 30 years old and lives in New York.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ef4b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputKeyToolsParser, JsonOutputToolsParser\n",
    "\n",
    "extraction_chain = prompt | extraction_llm_with_tools | JsonOutputToolsParser()\n",
    "extraction_chain.invoke({\"input\": \"John is 30 years old and he lives with his wife Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600b9e1",
   "metadata": {},
   "source": [
    "# Extract authors form a post from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0afc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nWhy We Think | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2025-05-01-thinking/\")\n",
    "docs = loader.load()\n",
    "doc = docs[0]\n",
    "doc.page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2593ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of the document\"\"\"\n",
    "    title: str = Field(description=\"Title of the document\")\n",
    "    summary: str = Field(description=\"Summary of the document\")\n",
    "    language: str = Field(description=\"Language of the document, e.g. en, it, es, etc.\")\n",
    "    keywords: list[str] = Field(description=\"keywords of the document\")\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Paper\"\"\"\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    author: str = Field(description=\"Author of the paper\")\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: list[Paper] = Field(description=\"Papers in the document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02e1cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'title': 'Why We Think | Lil',\n",
       "   'summary': 'This document discusses various aspects of thinking, including motivation, analogy to psychology, computation as a resource, and latent variable modeling. It also explores thinking in tokens, branching and editing, parallel sampling, sequential revision, RL for better reasoning, external tool use, thinking faithfully, optimization pressure on CoT: good or bad?, thinking in continuous space, recurrent architecture, thinking tokens, thinking as latent variables, expectation-maximization, iterative learning, scaling laws for thinking time, and future directions.',\n",
       "   'language': 'en',\n",
       "   'keywords': '[\"thinking\", \"motivation\", \"psychology\", \"computation\", \"latent variable modeling\", \"tokens\", \"branching\", \"editing\", \"parallel sampling\", \"sequential revision\", \"RL\", \"external tool use\", \"faithfulness\", \"optimization pressure\", \"CoT\", \"continuous space\", \"recurrent architecture\", \"thinking tokens\", \"expectation-maximization\", \"iterative learning\", \"scaling laws\"]'},\n",
       "  'type': 'Overview'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.output_parsers import JsonOutputToolsParser, JsonOutputKeyToolsParser\n",
    "\n",
    "doc_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"meta-llama-3.1-8b-instruct\", \n",
    "    api_key=\"dummy\",\n",
    "    temperature=0)\n",
    "\n",
    "overiew_llm = doc_llm.bind_tools([Overview])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Extract the overview of the document.'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "doc_chain = prompt | overiew_llm | JsonOutputToolsParser() # | PydanticToolsParser(tools=[Overview])\n",
    "doc_chain.invoke({\"input\": doc.page_content[:1000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ca47b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'args': {'papers': '[{\"title\": \"Motivation\", \"author\": \"Lilian Weng\"}, {\"title\": \"Analogy to Psychology\", \"author\": \"Lilian Weng\"}, {\"title\": \"Computation as a Resource\", \"author\": \"Lilian Weng\"}, {\"title\": \"Latent Variable Modeling\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking in Tokens\", \"author\": \"Lilian Weng\"}, {\"title\": \"Branching and Editing\", \"author\": \"Lilian Weng\"}, {\"title\": \"Parallel Sampling\", \"author\": \"Lilian Weng\"}, {\"title\": \"Sequential Revision\", \"author\": \"Lilian Weng\"}, {\"title\": \"RL for Better Reasoning\", \"author\": \"Lilian Weng\"}, {\"title\": \"External Tool Use\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking Faithfully\", \"author\": \"Lilian Weng\"}, {\"title\": \"Does the Model Tell What it Thinks Faithfully\", \"author\": \"Lilian Weng\"}, {\"title\": \"Optimization Pressure on CoT: Good or Bad?\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking in Continuous Space\", \"author\": \"Lilian Weng\"}, {\"title\": \"Recurrent Architecture\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking Tokens\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking as Latent Variables\", \"author\": \"Lilian Weng\"}, {\"title\": \"Expectation-Maximization\", \"author\": \"Lilian Weng\"}, {\"title\": \"Iterative Learning\", \"author\": \"Lilian Weng\"}, {\"title\": \"Scaling Laws for Thinking Time\", \"author\": \"Lilian Weng\"}, {\"title\": \"Whatâ€™s for Future\", \"author\": \"Lilian Weng\"}]'},\n",
       "  'type': 'Info'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info_llm = doc_llm.bind_tools([Info])\n",
    "\n",
    "system_message = \"\"\"\n",
    "An article will be passed to you. Extract from it all papers.\n",
    "DONOT extract the name of the article itself. If no papers are mentioned, return an empty list.\n",
    "DONOT make up or guess any information. Only extract what's written in the article.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_message),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "\n",
    "doc_chain = prompt | info_llm | JsonOutputToolsParser()\n",
    "doc_chain.invoke({\"input\": doc.page_content[:1000]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f85200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 24\n",
      "Average split length: 2380 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Increase chunk size to handle larger text while staying within token limits\n",
    "# With ~4 characters per token, 3000 characters â‰ˆ 750 tokens (safe for 4096 limit)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=200, chunk_size=3000)\n",
    "\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "print(f\"Average split length: {sum(len(s) for s in splits) / len(splits):.0f} characters\")\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7b7dbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# create a list of dicts with each split\n",
    "prep = RunnableLambda(lambda x: [{\"input\": split} for split in text_splitter.split_text(x)])\n",
    "prep.invoke('hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33289551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max ([len(input['input']) for input in prep.invoke(doc.page_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aacaf0b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 2 column 24 (char 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m json.loads(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m    [ \u001b[39m\u001b[33m{\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpapers\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m : \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtitle\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mThinking, Fast and Slow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mauthor\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mDaniel Kahneman\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m}, \u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtitle\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mTable of Contents\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mauthor\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mLilian Weng\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m}]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m }]\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder.decode(s)\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.raw_decode(s, idx=_w(s, \u001b[32m0\u001b[39m).end())\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.11/json/decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting ',' delimiter: line 2 column 24 (char 24)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.loads(\"\"\"\n",
    "    [ { \"papers\" : \"[{\\\"title\\\": \\\"Thinking, Fast and Slow\\\", \\\"author\\\": \\\"Daniel Kahneman\\\"}, {\\\"title\\\": \\\"Table of Contents\\\", \\\"author\\\": \\\"Lilian Weng\\\"}]\" }]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c759ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# create a list of dicts with each split\n",
    "prep = RunnableLambda(lambda x: [{\"input\": split} for split in text_splitter.split_text(x)])\n",
    "prep.invoke('hi')\n",
    "\n",
    "# Create a runnable to flatten list of lists\n",
    "# flatten_list = RunnableLambda(lambda x: sub_list['papers'] for sub_list in x)\n",
    "\n",
    "whole_doc_chain = prep | doc_chain.map() \n",
    "response = whole_doc_chain.invoke(doc.page_content[:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43a7e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"title\": \"Thinking, Fast and Slow\", \"author\": \"Daniel Kahneman\"}, {\"title\": \"Table of Contents\", \"author\": \"Lilian Weng\"}, {\"title\": \"Motivation\", \"author\": \"Lilian Weng\"}, {\"title\": \"Analogy to Psychology\", \"author\": \"Lilian Weng\"}, {\"title\": \"Computation as a Resource\", \"author\": \"Lilian Weng\"}, {\"title\": \"Latent Variable Modeling\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking in Tokens\", \"author\": \"Lilian Weng\"}, {\"title\": \"Branching and Editing\", \"author\": \"Lilian Weng\"}, {\"title\": \"Parallel Sampling\", \"author\": \"Lilian Weng\"}, {\"title\": \"Sequential Revision\", \"author\": \"Lilian Weng\"}, {\"title\": \"RL for Better Reasoning\", \"author\": \"Lilian Weng\"}, {\"title\": \"External Tool Use\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking Faithfully\", \"author\": \"Lilian Weng\"}, {\"title\": \"Does the Model Tell What it Thinks Faithfully\", \"author\": \"Lilian Weng\"}, {\"title\": \"Optimization Pressure on CoT: Good or Bad?\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking in Continuous Space\", \"author\": \"Lilian Weng\"}, {\"title\": \"Recurrent Architecture\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking Tokens\", \"author\": \"Lilian Weng\"}, {\"title\": \"Thinking as Latent Variables\", \"author\": \"Lilian Weng\"}, {\"title\": \"Expectation-Maximization\", \"author\": \"Lilian Weng\"}, {\"title\": \"Iterative Learning\", \"author\": \"Lilian Weng\"}, {\"title\": \"Scaling Laws for Thinking Time\", \"author\": \"Lilian Weng\"}, {\"title\": \"Whatâ€™s for Future\", \"author\": \"Lilian Weng\"}, {\"title\": \"Citation\", \"author\": \"Lilian Weng\"}, {\"title\": \"References\", \"author\": \"Lilian Weng\"}, {\"title\": \"Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\", \"author\": \"Lilian Weng\"}, {\"title\": \"Test time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions.\", \"author\": \"Lilian Weng\"}, {\"title\": \"Motivation#\", \"author\": \"Lilian Weng\"}, {\"title\": \"Enabling models to think for longer can be motivated in a few different ways.\", \"author\": \"Lilian Weng\"}, {\"title\": \"Analogy to Psychology#\", \"author\": \"Lilian Weng\"}, {\"title\": \"The core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \\\"What's 12345 times 56789?\\\".\", \"author\": \"Lilian Weng\"}, {\"title\": \"Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems.\", \"author\": \"Lilian Weng\"}, {\"title\": \"In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\", \"author\": \"Lilian Weng\"}, {\"title\": \"Fast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\", \"author\": \"Lilian Weng\"}, {\"title\": \"Slow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.\"}] <class 'str'>\n",
      "[{}] <class 'str'>\n",
      "[{}] <class 'str'>\n",
      "[{\"title\": \"Wei et al. 2022\", \"author\": \"Chain-of-thought prompting\"}, {\"title\": \"Wang et al. 2023\", \"author\": \"Self-consistency\"}, {\"title\": \"Kamoi et al. 2024\", \"author\": \"Huang et al. 2024\"}, {\"title\": \"Snell et al. 2024\", \"author\": \"\"}] <class 'str'>\n",
      "[{\"title\": \"Parallel Sampling vs Sequential Revision\", \"author\": \"Lightman et al. (2023)\"}, {\"title\": \"Illustration of parallel sampling vs sequential revision\", \"author\": \"Xie et al. (2023)\"]} <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove empty lists and flatten the results\n",
    "papers_list = []\n",
    "for response_record in response:\n",
    "    for record in response_record:\n",
    "        paper = record['args']['papers']\n",
    "        print(paper, type(paper))\n",
    "\n",
    "papers_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
