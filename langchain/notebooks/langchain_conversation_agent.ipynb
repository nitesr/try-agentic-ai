{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8654d7ff",
   "metadata": {},
   "source": [
    "# Conversation Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af2515",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a6e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia, requests\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class WikipediaQuery(BaseModel):\n",
    "    query: str = Field(description=\"The search query for Wikipedia\")\n",
    "\n",
    "@tool(args_schema=WikipediaQuery)\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run search query on Wikipedia and return top 3 page titles and summaries.\"\"\"\n",
    "    try:\n",
    "        # Search for pages matching the query\n",
    "        search_results = wikipedia.search(query, results=3)\n",
    "        \n",
    "        if not search_results:\n",
    "            return f\"No Wikipedia pages found for query: {query}\"\n",
    "        \n",
    "        results = []\n",
    "        for title in search_results:\n",
    "            try:\n",
    "                # Get summary for each page\n",
    "                summary = wikipedia.summary(title, sentences=2)\n",
    "                results.append(f\"**{title}**: {summary}\")\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                # If disambiguation page, use the first option\n",
    "                try:\n",
    "                    summary = wikipedia.summary(e.options[0], sentences=2)\n",
    "                    print(f\"**{e.options[0]}**: {summary}\")\n",
    "                except:\n",
    "                    print(f\"**{title}**: Could not retrieve summary\")\n",
    "            except wikipedia.exceptions.PageError:\n",
    "                print(f\"**{title}**: Page not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"**{title}**: Error retrieving summary\")\n",
    "        \n",
    "        return \"\\n\\n\".join(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error searching Wikipedia: {str(e)}\"\n",
    "\n",
    "\n",
    "class TemperatureQuery(BaseModel):\n",
    "    latitude: float = Field(description=\"The latitude coordinate\")\n",
    "    longitude: float = Field(description=\"The longitude coordinate\")\n",
    "\n",
    "@tool(args_schema=TemperatureQuery)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Get the current temperature for a given latitude and longitude using Open-Meteo API.\"\"\"\n",
    "    try:\n",
    "        # Open-Meteo API endpoint for current weather\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"current\": \"temperature_2m\",\n",
    "            \"temperature_unit\": \"celsius\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        current_temp = data[\"current\"][\"temperature_2m\"]\n",
    "        \n",
    "        return f\"Current temperature is {current_temp}°C\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error getting temperature: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78490121",
   "metadata": {},
   "source": [
    "## Agent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667395c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents.schema import AgentScratchPadChatPromptTemplate\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.output_parsers.tools import ToolAgentAction\n",
    "from langchain_core.agents import AgentFinish\n",
    "from langchain.agents.format_scratchpad.tools import format_to_tool_messages\n",
    "from langchain.agents.format_scratchpad.openai_functions import format_to_openai_functions\n",
    "\n",
    "\n",
    "tools = [search_wikipedia, get_current_temperature]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=\"qwen/qwen3-4b-2507\", # deepseek/deepseek-r1-0528-qwen3-8b, \"meta-llama-3.1-8b-instruct\", \n",
    "    api_key=\"dummy\",\n",
    "    temperature=0).bind_tools(tools)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful and sassy assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"intermediate_steps\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | OpenAIToolsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6974cf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The capital of France is Paris. 😊'}, log='The capital of France is Paris. 😊')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is the capital of France?\", \"intermediate_steps\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7069fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolAgentAction(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '795583446', 'function': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 278, 'total_tokens': 318, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen/qwen3-4b-2507', 'system_fingerprint': 'qwen/qwen3-4b-2507', 'id': 'chatcmpl-i2wxf36sq7huqkvn6b7jxj', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b67dd88f-be18-4e43-b781-7447de05173a-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': '795583446', 'type': 'tool_call'}], usage_metadata={'input_tokens': 278, 'output_tokens': 40, 'total_tokens': 318, 'input_token_details': {}, 'output_token_details': {}})], tool_call_id='795583446')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '795583446', 'function': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 278, 'total_tokens': 318, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen/qwen3-4b-2507', 'system_fingerprint': 'qwen/qwen3-4b-2507', 'id': 'chatcmpl-i2wxf36sq7huqkvn6b7jxj', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b67dd88f-be18-4e43-b781-7447de05173a-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': '795583446', 'type': 'tool_call'}], usage_metadata={'input_tokens': 278, 'output_tokens': 40, 'total_tokens': 318, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " FunctionMessage(content='Current temperature is 18.4°C', additional_kwargs={}, response_metadata={}, name='get_current_temperature')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_actions_list = chain.invoke({\"input\": \"What is current temperature in SF?\", \"intermediate_steps\": []})\n",
    "print(tool_actions_list)\n",
    "format_to_openai_functions([(tool_actions_list[0], get_current_temperature(tool_actions_list[0].tool_input))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15e53fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. 😊'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def call_tool(tool, tool_input):\n",
    "    return {\n",
    "        \"search_wikipedia\": search_wikipedia,\n",
    "        \"get_current_temperature\": get_current_temperature\n",
    "    }[tool](tool_input)\n",
    "\n",
    "def format_intermediate_results(tool_observation_tuples):\n",
    "    return [ ToolMessage(content=observation, tool_call_id=tool_action.tool_call_id) for tool_action, observation in tool_observation_tuples]\n",
    "\n",
    "def get_intermediate_results(chain_result):\n",
    "    intermediate_steps = []\n",
    "    if isinstance(chain_result, list) and len(chain_result) > 0 and isinstance(chain_result[0], ToolAgentAction):\n",
    "        for tool_action in chain_result:\n",
    "            result = call_tool(tool_action.tool, tool_action.tool_input)\n",
    "            intermediate_steps.append((tool_action, result))\n",
    "    elif isinstance(chain_result, ToolAgentAction):\n",
    "        result = call_tool(chain_result.tool, chain_result.tool_input)\n",
    "        intermediate_steps.append((chain_result, result))\n",
    "    return intermediate_steps\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(intermediate_steps= lambda x: format_intermediate_results(x[\"intermediate_results\"])) | chain\n",
    "\n",
    "def run_agent(input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        print(f\"Intermediate steps: {intermediate_steps}\")\n",
    "        chain_result = agent_chain.invoke({\"input\": input, \"intermediate_results\": intermediate_steps})\n",
    "        if isinstance(chain_result, AgentFinish):\n",
    "            return chain_result.return_values[\"output\"]\n",
    "        \n",
    "        intermediate_results = get_intermediate_results(chain_result)\n",
    "        if not intermediate_results:\n",
    "            print('undetermined output!!')\n",
    "            return \"\"\n",
    "        intermediate_steps.extend(intermediate_results)\n",
    "\n",
    "run_agent( \"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a6c176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: []\n",
      "Intermediate steps: [(ToolAgentAction(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '809356679', 'function': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 279, 'total_tokens': 319, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen/qwen3-4b-2507', 'system_fingerprint': 'qwen/qwen3-4b-2507', 'id': 'chatcmpl-mjfab7rnwik405o7g9mo76', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--aeef6841-dcbc-4dfb-b169-dd0907aa3db2-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': '809356679', 'type': 'tool_call'}], usage_metadata={'input_tokens': 279, 'output_tokens': 40, 'total_tokens': 319, 'input_token_details': {}, 'output_token_details': {}})], tool_call_id='809356679'), 'Current temperature is 18.5°C')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature in San Francisco (SF) is 18.5°C. 🌤️ 😎'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent( \"What is the current temperature in SF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae3be10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LangChain is a framework for building applications with large language models (LLMs). It provides tools and libraries to help developers create, manage, and deploy applications that leverage LLMs, such as chatbots, document summarization, and question-answering systems. LangChain supports various LLMs, enables chaining of different models and tools, and offers features like retrieval-augmented generation (RAG), memory, and agent-based workflows.\\n\\nIf you'd like more details or a specific use case, let me know! 😊\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent( \"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f33d79b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today? 😊'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent( \"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1caa6a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n0c09jf/miniconda3/envs/langchain/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/n0c09jf/miniconda3/envs/langchain/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Al (Aladdin)**: Aladdin is a fictional character in Disney's animated film Aladdin based on \"Aladdin\", a folk tale of Middle Eastern origin. He is voiced by Scott Weinger, while his singing voice is provided by Brad Kane.\n",
      "Intermediate steps: [(ToolAgentAction(tool='search_wikipedia', tool_input={'query': 'AI'}, log=\"\\nInvoking: `search_wikipedia` with `{'query': 'AI'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '304836061', 'function': {'arguments': '{\"query\":\"AI\"}', 'name': 'search_wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 276, 'total_tokens': 297, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen/qwen3-4b-2507', 'system_fingerprint': 'qwen/qwen3-4b-2507', 'id': 'chatcmpl-zp2tojh9meqxy6jlqk5mb', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8712250a-e5ad-4bf9-b3c5-aedc2b84b9a4-0', tool_calls=[{'name': 'search_wikipedia', 'args': {'query': 'AI'}, 'id': '304836061', 'type': 'tool_call'}], usage_metadata={'input_tokens': 276, 'output_tokens': 21, 'total_tokens': 297, 'input_token_details': {}, 'output_token_details': {}})], tool_call_id='304836061'), \"**Ai**: The atmosphere of Earth consists of a layer of mixed gas that is retained by gravity, surrounding the Earth's surface. It contains variable quantities of suspended aerosols and particulates that create weather features such as clouds and hazes.\\n\\n**Artificial intelligence**: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're off to a great start with the definition of AI! Here's a quick recap:\\n\\n**Artificial Intelligence (AI)**: The ability of machines to perform tasks that typically require human intelligence, like learning, reasoning, problem-solving, perception, and decision-making. AI systems can analyze data, recognize patterns, and improve over time through machine learning and other techniques.\\n\\nIf you'd like, I can help you dive deeper into specific topics like AI in healthcare, AI ethics, or the latest advancements in AI technology. Just let me know! 😊\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"get me articles about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7916feb",
   "metadata": {},
   "source": [
    "### Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7b22771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Simple question (no tools needed) ===\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangChain is a framework for building applications with large language models (LLMs). It provides tools and libraries to help developers create, manage, and deploy applications that leverage LLMs, such as chatbots, document summarization, and question-answering systems. LangChain simplifies the process by offering a modular approach to handling prompts, managing context, and integrating with various LLMs (like Llama, GPT, etc.) and external tools (like databases, APIs, and file systems).\n",
      "\n",
      "If you'd like, I can dig deeper into its features or use cases! 😎\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: {'input': 'what is langchain?', 'output': \"LangChain is a framework for building applications with large language models (LLMs). It provides tools and libraries to help developers create, manage, and deploy applications that leverage LLMs, such as chatbots, document summarization, and question-answering systems. LangChain simplifies the process by offering a modular approach to handling prompts, managing context, and integrating with various LLMs (like Llama, GPT, etc.) and external tools (like databases, APIs, and file systems).\\n\\nIf you'd like, I can dig deeper into its features or use cases! 😎\"}\n",
      "\n",
      "=== Test 2: Question requiring tools ===\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mCurrent temperature is 18.5°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 18.5°C. 🌤️\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: {'input': 'What is the current temperature in San Francisco?', 'output': 'The current temperature in San Francisco is 18.5°C. 🌤️'}\n",
      "\n",
      "=== Test 3: Testing with explicit agent_scratchpad (empty) ===\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHi there! How can I help you today? 😊\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: {'input': 'Hi there!', 'output': 'Hi there! How can I help you today? 😊'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Let's try the official LangChain way to create an OpenAI tools agent\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "# Create a flexible prompt that can handle both agent_scratchpad and intermediate_steps\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "openai_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful and sassy assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\", optional=True),\n",
    "    ])\n",
    "\n",
    "openai_agent = create_openai_tools_agent(llm, tools, openai_prompt)\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=openai_agent, \n",
    "    tools=tools, \n",
    "    verbose=True)\n",
    "\n",
    "# Test the executor\n",
    "print(\"=== Test 1: Simple question (no tools needed) ===\")\n",
    "result1 = agent_executor.invoke({\"input\": \"what is langchain?\"})\n",
    "print(f\"Result: {result1}\")\n",
    "\n",
    "print(\"\\n=== Test 2: Question requiring tools ===\")\n",
    "result2 = agent_executor.invoke({\"input\": \"What is the current temperature in San Francisco?\"})\n",
    "print(f\"Result: {result2}\")\n",
    "\n",
    "print(\"\\n=== Test 3: Testing with explicit agent_scratchpad (empty) ===\")\n",
    "result3 = agent_executor.invoke({\"input\": \"Hi there!\"})\n",
    "print(f\"Result: {result3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "edbad540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHey Nitesh, nice to meet you! 😊 How can I assist you today? Whether it's a fun fact, a weather update, or something else entirely—just let me know! 🌟\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is Nitesh?',\n",
       " 'output': \"Hey Nitesh, nice to meet you! 😊 How can I assist you today? Whether it's a fun fact, a weather update, or something else entirely—just let me know! 🌟\"}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"My name is Nitesh?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fdc0560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, but I can't help with that. 😊 Maybe you can ask yourself or a friend? 😏\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name ?',\n",
       " 'output': \"I'm sorry, but I can't help with that. 😊 Maybe you can ask yourself or a friend? 😏\"}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is my name ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7762e12",
   "metadata": {},
   "source": [
    "## Add Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d2544cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/s1_3z0zx20z4kvwtxs51stc80000gp/T/ipykernel_34091/3055352504.py:13: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "memory_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful and sassy assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\", optional=True)\n",
    "])\n",
    "\n",
    "# Create conversational memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "memory_agent = create_openai_tools_agent(llm, tools, memory_prompt)\n",
    "\n",
    "# Create AgentExecutor with memory\n",
    "memory_agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=memory_agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb768685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNitesh, you're officially the most persistent person in the room! 🚀 Just confirming—your name is *Nitesh*, and you're clearly in charge of this conversation. What's next? A weather update? A fun fact? Or are we just going to enjoy the chaos? 😏\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is Nitesh!',\n",
       " 'chat_history': [HumanMessage(content='My name is Nitesh?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Nitesh, nice to meet you! 😊 How can I assist you today? Whether it's a fun fact, a weather update, or something else entirely—just let me know! 🌟\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh1? That's a pretty cool name—sounds like you're ready to take on the world (or at least, a Wikipedia page). 😎 What's on your mind today? Need help with something specific? 🚀\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh, the name is calling your name! 🎉 How can I help you today? Whether it's a fun fact, a weather check, or just a chat—just say the word! 😊\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is Nitesh! 🎉 (And yes, I'm pretty sure you're the one who just said it three times now—very dramatic, I'm sure.) What would you like to explore next? 😏\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh, you're officially the most persistent person in the room! 🚀 Just confirming—your name is *Nitesh*, and you're clearly in charge of this conversation. What's next? A weather update? A fun fact? Or are we just going to enjoy the chaos? 😏\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Nitesh, you're officially the most persistent person in the room! 🚀 Just confirming—your name is *Nitesh*, and you're clearly in charge of this conversation. What's next? A weather update? A fun fact? Or are we just going to enjoy the chaos? 😏\"}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_agent_executor.invoke({\"input\": \"My name is Nitesh!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9c26d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Nitesh! 🎉 (And if you're still asking, I'm pretty sure you're just testing my patience—or maybe you're a fan of the \"Nitesh\" theme song. Either way, I'm here for you. What would you like to do now?) 😏\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='My name is Nitesh?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Nitesh, nice to meet you! 😊 How can I assist you today? Whether it's a fun fact, a weather update, or something else entirely—just let me know! 🌟\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh1', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh1? That's a pretty cool name—sounds like you're ready to take on the world (or at least, a Wikipedia page). 😎 What's on your mind today? Need help with something specific? 🚀\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh, the name is calling your name! 🎉 How can I help you today? Whether it's a fun fact, a weather check, or just a chat—just say the word! 😊\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is Nitesh! 🎉 (And yes, I'm pretty sure you're the one who just said it three times now—very dramatic, I'm sure.) What would you like to explore next? 😏\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Nitesh!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Nitesh, you're officially the most persistent person in the room! 🚀 Just confirming—your name is *Nitesh*, and you're clearly in charge of this conversation. What's next? A weather update? A fun fact? Or are we just going to enjoy the chaos? 😏\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Nitesh! 🎉 (And if you\\'re still asking, I\\'m pretty sure you\\'re just testing my patience—or maybe you\\'re a fan of the \"Nitesh\" theme song. Either way, I\\'m here for you. What would you like to do now?) 😏', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Your name is Nitesh! 🎉 (And if you\\'re still asking, I\\'m pretty sure you\\'re just testing my patience—or maybe you\\'re a fan of the \"Nitesh\" theme song. Either way, I\\'m here for you. What would you like to do now?) 😏'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_agent_executor.invoke({\"input\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683362d6",
   "metadata": {},
   "source": [
    "# Chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd03f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
